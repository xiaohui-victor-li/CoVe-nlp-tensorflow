{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing.download_preprocess import tokenize,UNK_ID, PAD_ID\n",
    "from utils import initialize_vocab, get_normalized_train_dir, f1, get_data_paths, exact_match\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "## Model hyperparameters\n",
    "tf.app.flags.DEFINE_string(\"model\", 'dcn', \"Model to train or evaluate, baseline / mixed / dcn / dcnplus\")\n",
    "tf.app.flags.DEFINE_string(\"cell\", 'lstm', \"Cell type to use for RNN, gru / lstm\")\n",
    "tf.app.flags.DEFINE_integer(\"embedding_size\", 300, \"Size of the pretrained vocabulary.\")\n",
    "tf.app.flags.DEFINE_integer(\"state_size\", 150, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_boolean(\"trainable_initial_state\", False, \"Make RNNCell initial states trainable.\")  # Not implemented\n",
    "tf.app.flags.DEFINE_boolean(\"trainable_embeddings\", False, \"Make embeddings trainable.\")\n",
    "tf.app.flags.DEFINE_float(\"input_keep_prob\", 0.7, \"Encoder: Fraction of units randomly kept of inputs to RNN.\")\n",
    "tf.app.flags.DEFINE_float(\"output_keep_prob\", 1.0, \"Encoder: Fraction of units randomly kept of outputs from RNN.\")\n",
    "tf.app.flags.DEFINE_float(\"state_keep_prob\", 1.0, \"Encoder: Fraction of units randomly kept of encoder states in RNN.\")\n",
    "tf.app.flags.DEFINE_float(\"encoding_keep_prob\", 1.0, \"Encoder: Fraction of encoding output kept.\")\n",
    "tf.app.flags.DEFINE_float(\"final_input_keep_prob\", 0.7, \"Encoder: Fraction of units randomly kept of inputs to final encoder RNN.\")\n",
    "## Data hyperparameters\n",
    "tf.app.flags.DEFINE_integer(\"max_question_length\", 25, \"Maximum question length.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_paragraph_length\", 400, \"Maximum paragraph length and the output size of your model.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size to use during training.\")\n",
    "\n",
    "\n",
    "# Mode\n",
    "tf.app.flags.DEFINE_string('mode', 'train', 'Mode to use, train/eval/shell/overfit')\n",
    "# Training hyperparameters\n",
    "tf.app.flags.DEFINE_integer(\"max_steps\", 50000, \"Steps until training loop stops.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.005, \"Learning rate.\")\n",
    "\n",
    "tf.app.flags.DEFINE_boolean(\"exponential_decay\", False, \"Whether to use exponential decay.\")\n",
    "tf.app.flags.DEFINE_float(\"decay_steps\", 4000, \"Number of steps for learning rate to decay by decay_rate\")\n",
    "tf.app.flags.DEFINE_boolean(\"staircase\", True, \"Whether staircase decay (use of integer division in decay).\")\n",
    "tf.app.flags.DEFINE_float(\"decay_rate\", 0.75, \"Learning rate.\")\n",
    "\n",
    "tf.app.flags.DEFINE_boolean(\"clip_gradients\", True, \"Whether to clip gradients.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 10.0, \"Clip gradients to this norm.\")\n",
    "\n",
    "# Evaluation arguments\n",
    "tf.app.flags.DEFINE_integer(\"eval_batches\", 80, \"Number of batches of size batch_size to use for evaluation.\")\n",
    "\n",
    "# Print\n",
    "tf.app.flags.DEFINE_integer(\"global_steps_per_timing\", 600, \"Number of steps per global step per sec evaluation.\")\n",
    "tf.app.flags.DEFINE_integer(\"print_every\", 200, \"How many iterations to do per print.\")\n",
    "\n",
    "# Directories etc.\n",
    "tf.app.flags.DEFINE_string(\"model_name\", datetime.now().strftime('%y-%m-Day%d-Hour%H-Min%M'), \"Models name, used for folder management.\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", os.path.join(os.curdir, \"data\", \"squad\"), \"SQuAD directory (default ../data/squad)\") \n",
    "tf.app.flags.DEFINE_string(\"train_dir\", os.path.join(os.curdir, \"train\"), \"Training directory to save the model parameters (default: ../checkpoints).\")\n",
    "tf.app.flags.DEFINE_string(\"vocab_path\", os.path.join(os.curdir, \"data\", \"squad\", \"vocab.dat\"), \"Path to vocab file (default: ../data/squad/vocab.dat)\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pipe = DataPipeline(*get_data_paths(FLAGS.data_dir),\n",
    "                          max_question_length=FLAGS.max_question_length, \n",
    "                          max_paragraph_length=FLAGS.max_paragraph_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
