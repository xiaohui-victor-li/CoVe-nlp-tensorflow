{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fine process the tokenized word sequence into token index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow import gfile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing.download_preprocess import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_dir = os.path.join(os.curdir, 'data', 'squad')\n",
    "\n",
    "train_path = pjoin(source_dir, \"train\")\n",
    "valid_path = pjoin(source_dir, \"val\")\n",
    "dev_path = pjoin(source_dir, \"dev\")\n",
    "\n",
    "# vocabulary\n",
    "vocab_path = pjoin(source_dir, \"vocab.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = \"<pad>\"\n",
    "_SOS = \"<sos>\"\n",
    "_UNK = \"<unk>\"\n",
    "_START_VOCAB = [_PAD, _SOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "SOS_ID = 1\n",
    "UNK_ID = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQuAD Vocubulary I/O\n",
    "total vocubulary token list from Question & Paragraph text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(\" \", space_separated_fragment.decode('utf-8')))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_paths, tokenizer):\n",
    "    '''Output'''\n",
    "    if not gfile.Exists(vocabulary_path):\n",
    "        print(\"creating vocabulary %s from data %s\" % (vocabulary_path, str(data_paths)))\n",
    "        vocab = {}\n",
    "        for path in tqdm(data_paths):\n",
    "            with open(path, mode=\"rb\") as f:\n",
    "                counter = 0\n",
    "                for line in f:\n",
    "                    counter += 1\n",
    "                    tokens = tokenizer(line)\n",
    "                    for w in tokens:\n",
    "                        if w in vocab:\n",
    "                            vocab[w] += 1\n",
    "                        else:\n",
    "                            vocab[w] = 1\n",
    "        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "        print(\"Vocabulary size: %d\" % len(vocab_list))\n",
    "        with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
    "            for w in vocab_list:\n",
    "                vocab_file.write(w + \"\\n\")\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "    '''Input'''\n",
    "    # map vocab to word embeddings\n",
    "    if gfile.Exists(vocabulary_path):\n",
    "        rev_vocab = []\n",
    "        with gfile.GFile(vocabulary_path, mode=\"r\") as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [line.strip('\\n') for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating vocabulary ./data/squad/vocab.dat from data ['./data/squad/train.context', './data/squad/train.question', './data/squad/val.context', './data/squad/val.question']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:17<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 115365\n"
     ]
    }
   ],
   "source": [
    "# store the vocabulary\n",
    "create_vocabulary(vocab_path,\n",
    "                  [pjoin(source_dir, \"train.context\"),\n",
    "                   pjoin(source_dir, \"train.question\"),\n",
    "                   pjoin(source_dir, \"val.context\"),\n",
    "                   pjoin(source_dir, \"val.question\")],\n",
    "                   basic_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the vocabulary\n",
    "vocab, rev_vocab = initialize_vocabulary(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_glove(vocab_list, save_path, glove_dir, glove_source,\n",
    "                   glove_dim=300, size=4e5, random_init=True):\n",
    "    \"\"\"\n",
    "    store the embedding matrix of specific GloVe vector \n",
    "    :param vocab_list: [vocab]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not gfile.Exists(save_path + \".npz\"):\n",
    "        if glove_source == 'wiki':\n",
    "            glove_path = os.path.join(glove_dir, \"glove.6B.{}d.txt\".format(glove_dim))\n",
    "        elif glove_source == 'crawl_cs':\n",
    "            glove_path = os.path.join(glove_dir, \"glove.840B.300d.txt\")\n",
    "            glove_dim = 300\n",
    "        elif glove_source == 'crawl_ci':\n",
    "            glove_path = os.path.join(glove_dir, \"glove.42B.300d.txt\")\n",
    "            glove_dim = 300\n",
    "        \n",
    "        if random_init:\n",
    "            glove = np.random.randn(len(vocab_list), glove_dim)\n",
    "        else:\n",
    "            glove = np.zeros((len(vocab_list), glove_dim))\n",
    "\n",
    "        found = 0\n",
    "        with open(glove_path, 'r', encoding='utf8') as fh:  # NOTE: encoding='utf8, new addition, may cause problems elsewhere\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.lstrip().rstrip().split(\" \")\n",
    "                word = array[0]\n",
    "                vector = list(map(float, array[1:]))\n",
    "                if word in vocab_list:\n",
    "                    idx = vocab_list.index(word)\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                elif word.capitalize() in vocab_list:\n",
    "                    idx = vocab_list.index(word.capitalize())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                elif word.lower() in vocab_list:\n",
    "                    idx = vocab_list.index(word.lower())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                elif word.upper() in vocab_list:\n",
    "                    idx = vocab_list.index(word.upper())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "\n",
    "        print(\"{}/{} of word vocab have corresponding vectors in {}\".format(found, len(vocab_list), glove_path))\n",
    "        np.savez_compressed(save_path, glove=glove)\n",
    "        print(\"saved trimmed glove matrix at: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000.0 [35:59<00:00, 185.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71734/115365 of word vocab have corresponding vectors in ./data/squad/glove.6B.300d.txt\n",
      "saved trimmed glove matrix at: ./data/squad/glove.trimmed.300\n"
     ]
    }
   ],
   "source": [
    "# store the glove word embeddingmatrix into glove.trimmed.300\n",
    "generate_glove(rev_vocab, source_dir + \"/glove.trimmed.300\", source_dir, 'wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the {train | dev | val} token id dataset\n",
    "* Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_token_ids(sentence, vocabulary, tokenizer):\n",
    "    words = tokenizer(sentence)\n",
    "    return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "\n",
    "def data_to_token_ids(data_path, target_path, vocabulary_path, tokenizer):\n",
    "    if not gfile.Exists(target_path):\n",
    "        print(\"Tokenizing data in %s\" % data_path)\n",
    "        vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "        with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
    "            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "                counter = 0\n",
    "                for line in data_file:\n",
    "                    counter += 1\n",
    "                    if counter % 5000 == 0:\n",
    "                        print(\"tokenizing line %d\" % counter)\n",
    "                    token_ids = sentence_to_token_ids(line, vocab, tokenizer)\n",
    "                    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data in ./data/squad/train.context\n",
      "tokenizing line 5000\n",
      "tokenizing line 10000\n",
      "tokenizing line 15000\n",
      "tokenizing line 20000\n",
      "tokenizing line 25000\n",
      "tokenizing line 30000\n",
      "tokenizing line 35000\n",
      "tokenizing line 40000\n",
      "tokenizing line 45000\n",
      "tokenizing line 50000\n",
      "tokenizing line 55000\n",
      "tokenizing line 60000\n",
      "tokenizing line 65000\n",
      "tokenizing line 70000\n",
      "tokenizing line 75000\n",
      "tokenizing line 80000\n",
      "Tokenizing data in ./data/squad/train.question\n",
      "tokenizing line 5000\n",
      "tokenizing line 10000\n",
      "tokenizing line 15000\n",
      "tokenizing line 20000\n",
      "tokenizing line 25000\n",
      "tokenizing line 30000\n",
      "tokenizing line 35000\n",
      "tokenizing line 40000\n",
      "tokenizing line 45000\n",
      "tokenizing line 50000\n",
      "tokenizing line 55000\n",
      "tokenizing line 60000\n",
      "tokenizing line 65000\n",
      "tokenizing line 70000\n",
      "tokenizing line 75000\n",
      "tokenizing line 80000\n"
     ]
    }
   ],
   "source": [
    "x_train_dis_path = train_path + \".ids.context\"\n",
    "y_train_ids_path = train_path + \".ids.question\"\n",
    "data_to_token_ids(train_path + \".context\", x_train_dis_path, vocab_path, basic_tokenizer)\n",
    "data_to_token_ids(train_path + \".question\", y_train_ids_path, vocab_path, basic_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Val Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data in ./data/squad/val.context\n",
      "Tokenizing data in ./data/squad/val.question\n"
     ]
    }
   ],
   "source": [
    "x_dis_path = valid_path + \".ids.context\"\n",
    "y_ids_path = valid_path + \".ids.question\"\n",
    "data_to_token_ids(valid_path + \".context\", x_dis_path, vocab_path, basic_tokenizer)\n",
    "data_to_token_ids(valid_path + \".question\", y_ids_path, vocab_path, basic_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
